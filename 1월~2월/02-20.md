# 1. 멀티노드 Kubernetes 클러스터 구축



![](https://velog.velcdn.com/images/luckyprice1103/post/b31cd8a4-f97a-46b1-8357-64ee6b896080/image.png)




## 1. 노드 생성.
### 1. ec2 템플릿 생성

원래는 ec2에 쿠버네티스, 도커 관련 설치를 각각 해야하지만, 템플릿을 이용하여 간소화 해보도록 함.

![](https://velog.velcdn.com/images/luckyprice1103/post/90cac7cf-8b0d-44d9-af42-cc8be23f1825/image.png)

운영체제 -> ubuntu 22.04
인스턴스 유형 -> t3.medium
스토리지 -> ebs gp2, 30gb


사용자 데이터에 설정 파일 입력.
![](https://velog.velcdn.com/images/luckyprice1103/post/cafa321e-11d3-417c-874b-ad4ad36d0c48/image.png)

```bash
#!/bin/bash
set -eux  # 에러 발생 시 중단 & 디버깅용 출력 활성화

# ✅ 패키지 업데이트
sudo apt-get update -y

# ✅ Swap 비활성화 (재부팅 후에도 유지)
sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab

# ✅ Swap 메모리 비활성화 확인
free -h

# ✅ 필수 패키지 설치
sudo apt-get install -y ca-certificates curl apt-transport-https gpg

# ✅ Docker GPG 키 추가
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

# ✅ Docker 저장소 추가 및 설치
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "${UBUNTU_CODENAME:-$VERSION_CODENAME}") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update
sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
sudo systemctl enable --now docker


# ✅ Containerd 관련 모듈 설정 (재부팅 후에도 유지됨)
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter

# ✅ Containerd 네트워크 설정 (Kubernetes와 호환되도록 수정)
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables=1
net.ipv4.ip_forward=1
net.bridge.bridge-nf-call-ip6tables=1
EOF
sudo sysctl --system

# ✅ containerd 설정 파일 수정
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml > /dev/null
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml
sudo systemctl restart containerd

# ✅ Kubernetes GPG 키 추가
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# ✅ Kubernetes 저장소 추가
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

# ✅ Kubernetes 설치 (kubeadm, kubelet, kubectl)
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl  # 버전 고정

# ✅ 설치 완료 메시지 출력
echo " Kubernetes & Docker launch complete now you can use them
```


생성 완료....!
![](https://velog.velcdn.com/images/luckyprice1103/post/240f18e7-4c60-4302-99b8-c6011708f3ac/image.png)




### 2. hostname변경
sudo su
sudo hostnamectl set-hostname kcontrolplane
sudo hostnamectl set-hostname worker01
sudo hostnamectl set-hostname worker02

### 3. 설치 확인.

sudo kubeadm init
#모든 terminal window마다해야함
export KUBECONFIG=/etc/kubernetes/admin.conf

💡 만약 permission denied 오류가 발생하면, 아래 권한 변경 추가
sudo chown $(id -u):$(id -g) /etc/kubernetes/admin.conf


- kcontrolplane
![](https://velog.velcdn.com/images/luckyprice1103/post/0fbec979-25dd-46fd-8354-374830556e61/image.png)


- worker node01
![](https://velog.velcdn.com/images/luckyprice1103/post/7dd056d8-a8c6-426d-8675-605ec99e399f/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/c04fdfd6-ee6f-4318-a6f1-1682a0d1af2a/image.png)



- worker node02
![](https://velog.velcdn.com/images/luckyprice1103/post/b8c74097-4f78-43cf-8921-eac5bc7d9a76/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/77b5d79f-f2d5-49f6-9a3d-f7c5ec3cb16e/image.png)

🚀 Worker 노드에서 kubectl version 실행 시 "The connection to the server localhost:8080 was refused" 오류 -> 정상

Worker 노드에서는 기본적으로 kubectl을 실행해도 Kubernetes API 서버(6443 포트)에 직접 접근할 수 없기 때문.

Worker 노드에는 Kubernetes Control Plane(API Server, etcd 등)이 없음 기본적으로 kubectl은 localhost:8080을 API 서버로 인식하지만, Worker 노드에는 API 서버가 없어서 연결이 실패 Worker 노드에서 kubectl을 실행하려면 Control Plane의 kubeconfig 파일을 복사해야 함

## 2. worker 노드를 Control Plane에 추가.


kcontrolplane에서 join message 복사 해서 worker로

==========worker =================================

sudo kubeadm join 192.168.35.50:6443 --token hm6os1.qoi3qn1hecpqx6pl --discovery-token-ca-cert-hash sha256:efe798a7234e5e3b6
![](https://velog.velcdn.com/images/luckyprice1103/post/3d2c1bd2-d4d7-49a3-bda7-938d9f3580b1/image.png)

- 에러
![](https://velog.velcdn.com/images/luckyprice1103/post/ed3f5795-62ee-4edc-9f38-fd6fc3330676/image.png)


AWS 보안 그룹은 기본적으로 자기 자신(SG 내부 통신)을 자동으로 허용하지 않기 때문에 허용해줘야함.

![](https://velog.velcdn.com/images/luckyprice1103/post/7091d5d6-431f-46d0-b033-584186db1fca/image.png)

- 성공.
![](https://velog.velcdn.com/images/luckyprice1103/post/a8bc3760-5e3b-4e83-bfb0-f4cc8565a494/image.png)

![](https://velog.velcdn.com/images/luckyprice1103/post/5017fd06-9c96-4fc7-b062-ecf126641508/image.png)

 이제 Worker01, Worker02 노드가 정상적으로 Control Plane에 합류됨
 
 ![](https://velog.velcdn.com/images/luckyprice1103/post/90e74db2-2281-4217-9ea9-71beb23e69a5/image.png)


## 3. network 설치

 이 단계에서는 Kubernetes 네트워크 설정을 적용하고, kubectl을 사용하여 클러스터의 상태를 점검.

==========master =================================

### 1. 네트워크 플러그인(Calico) 적용
#user
```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

![](https://velog.velcdn.com/images/luckyprice1103/post/e02f7074-9e42-44fd-b59b-78b6cd39875b/image.png)
✅ 이 코드가 하는 일:

Control Plane에서 관리자(Admin) 권한으로 kubectl을 실행할 수 있도록 Kubeconfig 설정을 복사하는 과정
/etc/kubernetes/admin.conf 파일을 현재 사용자($HOME/.kube/config)로 복사
kubectl이 이 설정을 자동으로 불러와서 Kubernetes 클러스터를 관리할 수 있도록 설정



#root


```bash
export KUBECONFIG=/etc/kubernetes/admin.conf
kubectl apply -f https://docs.projectcalico.org/v3.25/manifests/calico.yaml
```

![](https://velog.velcdn.com/images/luckyprice1103/post/e5843cbf-189c-46c0-aff2-9e38ea6f50f5/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/a010ef8e-5ede-44bf-946d-76a5ed4fae59/image.png)

✅ 이 코드가 하는 일:

kubectl이 기본적으로 /etc/kubernetes/admin.conf를 사용하도록 환경 변수 설정
현재 터미널 세션에서만 적용됨 (터미널을 닫으면 설정이 사라짐)
💡 이 설정이 없으면?

kubectl을 실행하면 Kubernetes API 서버에 연결할 수 없어서 kubectl get nodes 같은 명령어가 동작하지 않음

## 클러스터 동작 확인


```bash
kubectl get pods -A
```
![](https://velog.velcdn.com/images/luckyprice1103/post/91dc17fc-772a-44bb-aa92-9af28d7442a8/image.png)

✔ 모든 시스템 관련 Pod가 Running 상태인지 확인
✔ 특히 kube-system 네임스페이스에 있는 calico, kube-proxy가 정상 실행되는지 확인


```bash
kubectl get nodes
```

![](https://velog.velcdn.com/images/luckyprice1103/post/f90d4ecc-7dbf-423e-a36a-36f8c3edf4b3/image.png)

✔ 모든 노드가 Ready 상태라면 클러스터 정상 작동 🎉(NotReady->Ready)




# 2. 멀티 노드 클러스터에서 Prometheus + Grafana 모니터링 구축



[멀티노드 클러스터 구축.](https://velog.io/@luckyprice1103/%EB%A9%80%ED%8B%B0%EB%85%B8%EB%93%9C-Kubernetes-%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0-%EA%B5%AC%EC%B6%95)

✅ 목표:

Prometheus + Grafana를 Kubernetes 클러스터에 배포
클러스터의 CPU, 메모리 사용량을 모니터링하고 시각화
Helm을 사용하여 간편하게 설치

✅ 구성 요소:

Prometheus: Kubernetes의 메트릭 데이터를 수집하는 모니터링 시스템
Grafana: Prometheus에서 수집한 데이터를 시각화하는 대시보드


## 1. Helm 설치 (Helm 패키지 매니저 사용)
🔹 Helm이란?

Kubernetes에서 어플리케이션을 간편하게 설치할 수 있는 패키지 관리 도구
Prometheus + Grafana를 Helm Chart로 쉽게 설치할 수 있음

🔹 Helm 설치:
Control Plane에서 실행
``` bash
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

helm version

```



## 2. Prometheus + Grafana를 Helm으로 설치
🔹 Helm을 사용하여 Prometheus & Grafana Chart 저장소 추가
```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
```
✔ prometheus-community라는 Helm Chart 저장소가 추가됨


🔹 Prometheus + Grafana 배포
```bash
helm install monitoring prometheus-community/kube-prometheus-stack --namespace monitoring --create-namespace

```
![](https://velog.velcdn.com/images/luckyprice1103/post/f776fae3-296c-4a6f-830d-93aaeae2c2c5/image.png)


✅ 이 명령어가 하는 일:

Prometheus + Grafana를 monitoring 네임스페이스에 배포
kube-prometheus-stack Chart를 사용하여 Kubernetes 클러스터 모니터링


✔ 설치가 완료되었는지 확인:

``` bash
kubectl get pods -n monitoring

```
![](https://velog.velcdn.com/images/luckyprice1103/post/eb474ffa-af35-426f-ae7b-a1b815077a6f/image.png)
✅ Running 상태면 성공적으로 배포된 것! 🎉


## 3. Grafana에 접속하여 클러스터 모니터링

### 1. 포트 3000 웹 접속
🔹 Grafana를 포트 포워딩하여 웹 브라우저에서 접근 가능하게 만들기
``` bash
kubectl port-forward svc/monitoring-grafana 3000:80 -n monitoring
```
✅ 클러스터 내부의 Grafana(포트 80)를 로컬 컴퓨터의 localhost:3000에서 접속할 수 있도록 포트 포워딩하는 역할을 함.



✅ 웹 브라우저에서 접속 시도:

![](https://velog.velcdn.com/images/luckyprice1103/post/e83ab11f-7ae6-4686-99d3-d0d380eba1f4/image.png)
🔹 연결 실패.

kubectl port-forward는 로컬 머신에서만 접속 가능. kubectl port-forward는 오직 localhost(127.0.0.1)에서만 접근 가능!
즉, 퍼블릭 IP(예: 3.39.236.1:3000 로는 접근할 수 없음!)

💡 해결 방법:

SSH 터널링을 이용해서 로컬에서 접속하기

=================내 로컬 컴퓨터=======================
``` bash
ssh -L 3000:localhost:3000 -N -f -i kakao_key.pem ubuntu@ec2-3-39-236-1.ap-northeast-2.compute.amazonaws.com
```
![](https://velog.velcdn.com/images/luckyprice1103/post/80f57b13-3302-458f-89c8-7a6d8958122d/image.png)


🔹다시 접속 시도. (내 로컬 컴퓨터의 localhost:3000)
✅ 이제 웹 브라우저에서 접속 성공:

![](https://velog.velcdn.com/images/luckyprice1103/post/fc878426-c502-416b-a686-3a7b3177dd85/image.png)

🔹 Grafana 기본 로그인 정보:

Username: admin
Password: prom-operator (기본값)


### 2. 클러스터 모니터링 테스트

✔ 로그인 후 "Add Data Source" → "Prometheus" 선택
✔ Prometheus 주소를 http://monitoring-prometheus-kube-prometheus:9090로 설정
![](https://velog.velcdn.com/images/luckyprice1103/post/da0fddb8-8d9d-4c3f-a7a3-e4229a18b7ba/image.png)


1️⃣ Grafana 왼쪽 메뉴에서 📊 Dashboards → Import 클릭
2️⃣ 공식 Kubernetes 모니터링 대시보드 ID 입력

대시보드 ID: 315 (Kubernetes 클러스터 모니터링)
또는 1860 (Kubernetes Node Exporter)
3️⃣ "Load" 버튼 클릭
4️⃣ "Prometheus" 데이터 소스 선택
5️⃣ "Import" 버튼 클릭
✅ 이제 Kubernetes 클러스터의 전체적인 리소스 모니터링 가능...! 🎉

![](https://velog.velcdn.com/images/luckyprice1103/post/b46f656a-8312-4317-b5c1-c1175311f171/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/7e2f64dd-580e-4787-bd38-2a85d93b896c/image.png)


## 4. 클러스터 부하 테스트

### 1. BusyBox를 사용하여 CPU 부하 발생
📌 Control Plane에서 실행:
```bash
kubectl run cpu-stress --image=busybox --restart=Never -- /bin/sh -c "while true; do echo 'stressing CPU'; done"

```
✅ 이 Pod는 종료되지 않고 계속해서 CPU 부하를 발생시킴.

🔹 실행된 Pod 확인:

```
kubectl get pods
```
🔹 특정 Pod의 로그 확인 (CPU 부하가 지속적으로 발생하는지 확인)

```
kubectl logs cpu-stress
```
✔ CPU 부하를 실시간으로 모니터링하려면 Prometheus + Grafana 대시보드를 확인!
![](https://velog.velcdn.com/images/luckyprice1103/post/e37fc02a-95a8-43a5-be94-8e74e7d2a605/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/0f4f2fb0-e0d8-45a0-a3c8-82059e2f36d7/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/859509ca-7634-4fd5-886a-b7752a34f87a/image.png)

✅ 부하 테스트 종료:

```
kubectl delete pod cpu-stress
```


### 2. 멀티노드에서 HTTP 부하 테스트 (k6 사용)
✅ 목표:

여러 Worker 노드에서 HTTP 요청을 병렬로 실행하여 실제 트래픽 부하 테스트 진행
k6는 Kubernetes에서 여러 Pod로 배포 가능하여 멀티노드 테스트 가능

✔ 1) k6 부하 테스트를 실행할 Deployment 생성

- k6-deployment.yaml
```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k6-load-test
spec:
  replicas: 3  # 여러 Worker 노드에서 실행되도록 설정
  selector:
    matchLabels:
      app: k6
  template:
    metadata:
      labels:
        app: k6
    spec:
      containers:
      - name: k6
        image: grafana/k6
        args: ["run", "-u", "50", "-d", "60s", "https://nginx:80"]

```

📌 적용:


```bash
kubectl apply -f k6-deployment.yam
```

✅ 설명:

replicas: 3 → 3개의 Pod가 여러 Worker 노드에서 배포되며 HTTP 부하를 분산 생성
-u 50 → 동시 50명의 사용자(virtual users) 생성
-d 60s → 60초 동안 HTTP 요청을 지속적으로 수행

![](https://velog.velcdn.com/images/luckyprice1103/post/52e71d70-4369-45e0-b6a1-7ecc363a56ec/image.png)


![](https://velog.velcdn.com/images/luckyprice1103/post/98691c5e-bc23-4705-89db-2f3c9c89f58d/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/7646d1eb-2b3f-47c2-b05b-3f3d8241eeb1/image.png)


kubectl delete deployment k6-load-test

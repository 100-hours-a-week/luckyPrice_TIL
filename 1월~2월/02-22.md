# 멀티 노드 클러스터에서 Prometheus + Grafana 모니터링 구축



[멀티노드 클러스터 구축.](https://velog.io/@luckyprice1103/%EB%A9%80%ED%8B%B0%EB%85%B8%EB%93%9C-Kubernetes-%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0-%EA%B5%AC%EC%B6%95)

✅ 목표:

Prometheus + Grafana를 Kubernetes 클러스터에 배포
클러스터의 CPU, 메모리 사용량을 모니터링하고 시각화
Helm을 사용하여 간편하게 설치

✅ 구성 요소:

Prometheus: Kubernetes의 메트릭 데이터를 수집하는 모니터링 시스템
Grafana: Prometheus에서 수집한 데이터를 시각화하는 대시보드


## 1. Helm 설치 (Helm 패키지 매니저 사용)
🔹 Helm이란?

Kubernetes에서 어플리케이션을 간편하게 설치할 수 있는 패키지 관리 도구
Prometheus + Grafana를 Helm Chart로 쉽게 설치할 수 있음

🔹 Helm 설치:
Control Plane에서 실행
``` bash
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

helm version

```



## 2. Prometheus + Grafana를 Helm으로 설치
🔹 Helm을 사용하여 Prometheus & Grafana Chart 저장소 추가
```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
```
✔ prometheus-community라는 Helm Chart 저장소가 추가됨


🔹 Prometheus + Grafana 배포
```bash
helm install monitoring prometheus-community/kube-prometheus-stack --namespace monitoring --create-namespace

```
![](https://velog.velcdn.com/images/luckyprice1103/post/f776fae3-296c-4a6f-830d-93aaeae2c2c5/image.png)


✅ 이 명령어가 하는 일:

Prometheus + Grafana를 monitoring 네임스페이스에 배포
kube-prometheus-stack Chart를 사용하여 Kubernetes 클러스터 모니터링


✔ 설치가 완료되었는지 확인:

``` bash
kubectl get pods -n monitoring

```
![](https://velog.velcdn.com/images/luckyprice1103/post/eb474ffa-af35-426f-ae7b-a1b815077a6f/image.png)
✅ Running 상태면 성공적으로 배포된 것! 🎉


## 3. Grafana에 접속하여 클러스터 모니터링

### 1. 포트 3000 웹 접속
🔹 Grafana를 포트 포워딩하여 웹 브라우저에서 접근 가능하게 만들기
``` bash
kubectl port-forward svc/monitoring-grafana 3000:80 -n monitoring
```
✅ 클러스터 내부의 Grafana(포트 80)를 로컬 컴퓨터의 localhost:3000에서 접속할 수 있도록 포트 포워딩하는 역할을 함.



✅ 웹 브라우저에서 접속 시도:

![](https://velog.velcdn.com/images/luckyprice1103/post/e83ab11f-7ae6-4686-99d3-d0d380eba1f4/image.png)
🔹 연결 실패.

kubectl port-forward는 로컬 머신에서만 접속 가능. kubectl port-forward는 오직 localhost(127.0.0.1)에서만 접근 가능!
즉, 퍼블릭 IP(예: 3.39.236.1:3000 로는 접근할 수 없음!)

💡 해결 방법:

SSH 터널링을 이용해서 로컬에서 접속하기

=================내 로컬 컴퓨터=======================
``` bash
ssh -L 3000:localhost:3000 -N -f -i kakao_key.pem ubuntu@ec2-3-39-236-1.ap-northeast-2.compute.amazonaws.com
```
![](https://velog.velcdn.com/images/luckyprice1103/post/80f57b13-3302-458f-89c8-7a6d8958122d/image.png)


🔹다시 접속 시도. (내 로컬 컴퓨터의 localhost:3000)
✅ 이제 웹 브라우저에서 접속 성공:

![](https://velog.velcdn.com/images/luckyprice1103/post/fc878426-c502-416b-a686-3a7b3177dd85/image.png)

🔹 Grafana 기본 로그인 정보:

Username: admin
Password: prom-operator (기본값)


### 2. 클러스터 모니터링 테스트

✔ 로그인 후 "Add Data Source" → "Prometheus" 선택
✔ Prometheus 주소를 http://monitoring-prometheus-kube-prometheus:9090로 설정
![](https://velog.velcdn.com/images/luckyprice1103/post/da0fddb8-8d9d-4c3f-a7a3-e4229a18b7ba/image.png)


1️⃣ Grafana 왼쪽 메뉴에서 📊 Dashboards → Import 클릭
2️⃣ 공식 Kubernetes 모니터링 대시보드 ID 입력

대시보드 ID: 315 (Kubernetes 클러스터 모니터링)
또는 1860 (Kubernetes Node Exporter)
3️⃣ "Load" 버튼 클릭
4️⃣ "Prometheus" 데이터 소스 선택
5️⃣ "Import" 버튼 클릭
✅ 이제 Kubernetes 클러스터의 전체적인 리소스 모니터링 가능...! 🎉

![](https://velog.velcdn.com/images/luckyprice1103/post/b46f656a-8312-4317-b5c1-c1175311f171/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/7e2f64dd-580e-4787-bd38-2a85d93b896c/image.png)


## 4. 클러스터 부하 테스트

### 1. BusyBox를 사용하여 CPU 부하 발생
📌 Control Plane에서 실행:
```bash
kubectl run cpu-stress --image=busybox --restart=Never -- /bin/sh -c "while true; do echo 'stressing CPU'; done"

```
✅ 이 Pod는 종료되지 않고 계속해서 CPU 부하를 발생시킴.

🔹 실행된 Pod 확인:

```
kubectl get pods
```
🔹 특정 Pod의 로그 확인 (CPU 부하가 지속적으로 발생하는지 확인)

```
kubectl logs cpu-stress
```
✔ CPU 부하를 실시간으로 모니터링하려면 Prometheus + Grafana 대시보드를 확인!
![](https://velog.velcdn.com/images/luckyprice1103/post/e37fc02a-95a8-43a5-be94-8e74e7d2a605/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/0f4f2fb0-e0d8-45a0-a3c8-82059e2f36d7/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/859509ca-7634-4fd5-886a-b7752a34f87a/image.png)

✅ 부하 테스트 종료:

```
kubectl delete pod cpu-stress
```


### 2. 멀티노드에서 HTTP 부하 테스트 (k6 사용)
✅ 목표:

여러 Worker 노드에서 HTTP 요청을 병렬로 실행하여 실제 트래픽 부하 테스트 진행
k6는 Kubernetes에서 여러 Pod로 배포 가능하여 멀티노드 테스트 가능

✔ 1) k6 부하 테스트를 실행할 Deployment 생성

- k6-deployment.yaml
```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k6-load-test
spec:
  replicas: 3  # 여러 Worker 노드에서 실행되도록 설정
  selector:
    matchLabels:
      app: k6
  template:
    metadata:
      labels:
        app: k6
    spec:
      containers:
      - name: k6
        image: grafana/k6
        args: ["run", "-u", "50", "-d", "60s", "https://nginx:80"]

```

📌 적용:


```bash
kubectl apply -f k6-deployment.yam
```

✅ 설명:

replicas: 3 → 3개의 Pod가 여러 Worker 노드에서 배포되며 HTTP 부하를 분산 생성
-u 50 → 동시 50명의 사용자(virtual users) 생성
-d 60s → 60초 동안 HTTP 요청을 지속적으로 수행

![](https://velog.velcdn.com/images/luckyprice1103/post/52e71d70-4369-45e0-b6a1-7ecc363a56ec/image.png)


![](https://velog.velcdn.com/images/luckyprice1103/post/98691c5e-bc23-4705-89db-2f3c9c89f58d/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/7646d1eb-2b3f-47c2-b05b-3f3d8241eeb1/image.png)


kubectl delete deployment k6-load-test

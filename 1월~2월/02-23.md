## 1단계: 쿠버네티스 클러스터 구축 및 애플리케이션 배포
목표:
쿠버네티스 클러스터를 구축하고, 웹 애플리케이션을 컨테이너 기반으로 배포한 후, 서비스(Service) 설정을 통해 외부에서 접근 가능하도록 구성


✅ 수행 과정
쿠버네티스 클러스터 구축



### 1. AWS EC2 기반의 멀티노드 클러스터 Kubeadm 구축

[멀티 노드 클러스터 구축.](https://velog.io/@luckyprice1103/%EB%A9%80%ED%8B%B0%EB%85%B8%EB%93%9C-Kubernetes-%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0-%EA%B5%AC%EC%B6%95)

노드 확장을 고려한 구조 설계 및 네트워크 구성

✅ 1. 노드 확장 고려 여부 점검
![](https://velog.velcdn.com/images/luckyprice1103/post/2447f1e7-e142-47b6-a555-a0550cd0ed4b/image.png)


✅ 2. 네트워크 구성 검토 및 최적화
![](https://velog.velcdn.com/images/luckyprice1103/post/fe628311-7b44-429f-93ea-b9310c9965fd/image.png)




### 2. 애플리케이션 컨테이너 배포

개발팀에서 제공한 Nginx 또는 Node.js 기반 컨테이너 이미지 배포
Deployment 생성 및 다중 파드 배포

(1) Deployment 매니페스트 작성
Deployment를 이용해 애플리케이션을 컨테이너 기반으로 배포.

(app-deployment.yaml)

``` yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-web-app
  labels:
    app: my-web-app
spec:
  replicas: 3  # 3개의 파드를 생성
  selector:
    matchLabels:
      app: my-web-app
  template:
    metadata:
      labels:
        app: my-web-app
    spec:
      containers:
        - name: my-web-app
          image: nginx:latest  # 개발팀 제공 이미지 사용 가능
          ports:
            - containerPort: 80
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
```

replicas: 3 → 파드를 3개 배포하여 부하를 분산
image: nginx:latest → 최신 Nginx 이미지를 사용
containerPort: 80 → 웹 서비스 포트 설정
resources → CPU 및 메모리 사용량 설정 (적절한 값으로 조정 가능)

- t3.medium의 기본적인 스펙:
vCPU: 2개
RAM: 4GB (4096Mi)
네트워크 성능: 중간
EBS 대역폭: 5Gbps
두 개의 워커 노드를 사용하므로 총 가용 자원은:
CPU: 2vCPU × 2 = 4vCPU
메모리: 4GB × 2 = 8GB



- 각 파드의 최소 요구량(Requests)
CPU: 100m (0.1 vCPU)
메모리: 128Mi
- 각 파드의 최대 사용 가능량(Limits)
CPU: 500m (0.5 vCPU)
메모리: 512Mi

- 3개의 파드가 최소 요구량만 사용하는 경우
CPU: 100m × 3 = 300m (0.3 vCPU)
메모리: 128Mi × 3 = 384Mi
💡 이 경우, t3.medium 노드 하나(2 vCPU, 4GB)로 충분히 감당 가능
💡 즉, 하나의 노드에서도 충분히 운영 가능하며, 두 개의 노드가 있다면 여유롭게 운영됨

- 3개의 파드가 최대 Limits까지 사용하는 경우
CPU: 500m × 3 = 1500m (1.5 vCPU)
메모리: 512Mi × 3 = 1536Mi
💡 t3.medium 하나의 2 vCPU와 4GB 내에서 운영 가능
💡 노드가 2개이므로 총 4 vCPU / 8GB 메모리로 충분한 여유 공간 확보

(2) Deployment 배포.

``` bash
kubectl apply -f app-deployment.yaml
```
![](https://velog.velcdn.com/images/luckyprice1103/post/193aaf80-280c-4f11-8413-e266b5ce769e/image.png)


kubectl scale 명령어를 사용하여 replicas 개수를 조정


``` bash
kubectl scale deployment my-web-app --replicas=4
```

![](https://velog.velcdn.com/images/luckyprice1103/post/a6b8e6ce-7625-473e-894e-415b6a385255/image.png)
-> 4개로 늘어남.

### 3. 서비스 설정 및 트래픽 관리

로드 밸런싱을 위한 Service (LoadBalancer / Ingress) 설정
외부에서 애플리케이션 접근 테스트 및 네트워크 문제 해결

service-nodeport.yaml
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-web-service
spec:
  selector:
    app: my-web-app
  type: NodePort  # 외부에서 접근 가능하도록 설정
  ports:
    - protocol: TCP
      port: 80          # 서비스에서 사용할 포트
      targetPort: 80    # 컨테이너 내부 포트
      nodePort: 30080   # 노드에서 접근할 포트 (30000~32767 사이)

```

```bash
kubectl apply -f service-nodeport.yaml
```

![](https://velog.velcdn.com/images/luckyprice1103/post/31b9325f-6967-4fc9-be35-eb7dcd092bab/image.png)


외부 접속 성공...!(보안 그룹에서도 포트 열어주기.)
![](https://velog.velcdn.com/images/luckyprice1103/post/fa638049-8d3a-4d46-8626-07d6e617afb3/image.png)



service-loadbalancer.yaml
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-web-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
spec:
  selector:
    app: my-web-app
  type: LoadBalancer  # 클라우드 환경에서는 LoadBalancer 자동 생성됨
  ports:
    - protocol: TCP
      port: 80          # 서비스에서 사용할 포트
      targetPort: 80    # 컨테이너 내부 포트


```

```yaml
kubectl apply -f service-loadbalancer.yaml
kubectl get svc

```

![](https://velog.velcdn.com/images/luckyprice1103/post/aa91ba7d-7b1d-4657-bcc5-18d76ec620cb/image.png)

- external-ip가 pending에서 멈춰있음.-> loadbalacner를 못 만드는 상태


💡 AWS LoadBalancer Controller를 사용하려면, EC2 인스턴스에 IAM Role을 부여해야 함.




✅ 1️⃣ 환경 설정 및 준비
✔ 멀티 노드 클러스터 구축 (1 Master, 2 Worker)
✔ EC2 IAM 역할 부여 (LoadBalancer Controller 사용 가능하도록 설정)

AWS 콘솔에서 IAM 역할 생성

IAM → Roles → Create Role 클릭
EC2 선택 → Next
정책 추가 → AdministratorAccess 선택 (테스트용)
역할 이름: K8s-EC2-Role
Create Role 완료
EC2 인스턴스에 IAM Role 할당

AWS 콘솔 → EC2 → 해당 인스턴스 선택
"Actions" → "Security" → "Modify IAM Role" 클릭
IAM Role을 K8s-EC2-Role로 변경
저장 후 적용

✔ AWS Load Balancer Controller 설치 (Helm 사용)

✅ 2️⃣ 문제 발생 & 해결 과정
🚨 문제 1: LoadBalancer 생성되지만 External IP <pending> 상태
❌ 원인: AWS Load Balancer Controller가 올바른 서브넷을 찾지 못함
🔧 해결: Public Subnet을 kubernetes.io/role/elb=1 태그로 설정


```bash
aws ec2 create-tags --resources <SUBNET_ID> --tags Key=kubernetes.io/role/elb,Value=1
```
🚨 문제 2: Target Group이 Worker 노드를 등록하지 않음
❌ 원인: Worker 노드에 providerID가 설정되지 않음
🔧 해결: Worker 노드에 providerID 추가


```bash
kubectl patch node worker01 -p '{"spec":{"providerID":"aws://ap-northeast-2a/i-02655819459c13fc2"}}'
kubectl patch node worker02 -p '{"spec":{"providerID":"aws://ap-northeast-2b/i-0e99432702999f559"}}'
```
🔧 kubelet 재시작 후 확인


```bash
sudo systemctl restart kubelet
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{" → "}{.spec.providerID}{"\n"}{end}'
```
🚨 문제 3: LoadBalancer는 생성되었지만 접속이 안됨
❌ 원인:

Target Group이 Worker 노드를 unhealthy 상태로 인식

✅ Target Group 상태 확인 & Worker 노드가 healthy인지 체크

```bash
  aws elbv2 describe-load-balancers --query "LoadBalancers[*].{Name:LoadBalancerName,DNS:DNSName,ARN:LoadBalancerArn}" --output json
aws elbv2 describe-target-groups --query "TargetGroups[*].{Name:TargetGroupName,ARN:TargetGroupArn}" --output json
```

  
```bash
aws elbv2 describe-target-health --target-group-arn <TARGET_GROUP_ARN>
aws elbv2 describe-target-health --target-group-arn arn:aws:elasticloadbalancing:ap-northeast-2:509399622222:targetgroup/k8s-default-mywebser-7ea040c771/e96dae729e11a684
  aws elbv2 describe-target-groups --query "TargetGroups[?TargetGroupName=='k8s-default-mywebser-7ea040c771']"
```
✅ LoadBalancer Controller 재시작

```bash
kubectl delete pod -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller
```
✅ 3️⃣ 최종 확인 & 성공 🎉
✔ kubectl get svc my-web-service 실행 시 External IP 생성됨
✔ LoadBalancer DNS로 접속 가능
✔ Target Group에서 Worker 노드가 healthy 상태

![](https://velog.velcdn.com/images/luckyprice1103/post/906768b7-33cd-47f5-b0f5-2069c187f916/image.png)

  
  ### 4. Ingress 설정 및 외부 접근 구성
  
  ✅ 목표:

AWS **NLB(Network Load Balancer)**를 사용하여 Ingress Controller와 연결
Ingress를 통해 들어온 트래픽을 내부 서비스(Nginx)로 전달
  
>   사용자 (HTTP 요청)
      ↓
AWS LoadBalancer (NLB)
      ↓
Service (type=LoadBalancer, Ingress Controller가 연결됨)
      ↓
Ingress (트래픽을 어디로 보낼지 결정)
      ↓
Service (type=ClusterIP)
      ↓
Pod 1 (192.168.5.42:80)
Pod 2 (192.168.5.43:80)
Pod 3 (192.168.5.44:80)
Pod 4 (192.168.5.45:80)

  
  📌 (1) NGINX Ingress Controller 설치


```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
```
  
  ![](https://velog.velcdn.com/images/luckyprice1103/post/9ca4c2a6-b7e4-4388-9968-7b2b70f235f2/image.png)
  
 
📌 (2) 기존 LoadBalancer를 Ingress와 연결
Ingress가 기존 LoadBalancer를 사용하도록 설정하려면, Ingress Controller에 NodePort 방식으로 노출하여 LoadBalancer가 Ingress 트래픽을 처리하도록 해야 합니다.

🔹 Ingress Controller를 NodePort로 변경


```bash
kubectl patch svc ingress-nginx-controller -n ingress-nginx -p '{"spec": {"type": "NodePort"}}'
```
```bash
kubectl get svc -n ingress-nginx
```
출력 예시:

![](https://velog.velcdn.com/images/luckyprice1103/post/b5127e6c-237a-43bc-a3da-96dc4f68ea25/image.png)

✔ NodePort 방식으로 노출됨
✔ 포트 32100과 32200이 자동 할당됨

  
할당됨

📌 (3) 기존 LoadBalancer가 Ingress로 트래픽 전달하도록 변경
기존 LoadBalancer에서 Ingress Controller의 NodePort로 트래픽을 전달합니다.

🔹 기존 LoadBalancer 서비스 변경
  ![](https://velog.velcdn.com/images/luckyprice1103/post/255d3880-3e02-4da3-9ec3-63e54fb75e2b/image.png)
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-web-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
spec:
  selector:
    app: my-web-app
  type: LoadBalancer  # 클라우드 환경에서는 LoadBalancer 자동 생성됨
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 32100  # Ingress Controller의 HTTP NodePort
    - name: https
      protocol: TCP
      port: 443
      targetPort: 32200  # Ingress Controller의 HTTPS NodePort
  selector:
    app.kubernetes.io/name: ingress-nginx
```

✔ 이 설정을 적용하면:

기존 LoadBalancer (my-web-loadbalancer) 가 Ingress Controller의 NodePort로 트래픽을 전달
Ingress가 도메인 기반 트래픽을 서비스로 라우팅
Ingress가 LoadBalancer의 EXTERNAL-IP를 그대로 사용

```bash
kubectl apply -f my-web-loadbalancer.yaml
kubectl get svc
```
EXTERNAL-IP가 기존 LoadBalancer와 동일해야 함.
  
  
📌 (4) Ingress 리소스 생성
이제 Ingress를 설정하여 도메인 기반 트래픽을 서비스로 라우팅.

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-web-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
    - host: myweb.local
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: my-web-service
                port:
                  number: 80
```
✔ 핵심 내용

ingressClassName: nginx → NGINX Ingress Controller 사용
host: myweb.local → 도메인 기반 트래픽 라우팅
backend.service.name: my-web-service → 트래픽을 my-web-service로 전달

```bash
kubectl apply -f ingress.yaml
kubectl get ingress
```

  ![](https://velog.velcdn.com/images/luckyprice1103/post/ad6987bf-95a3-4e47-9211-66c51c6d256e/image.png)

![](https://velog.velcdn.com/images/luckyprice1103/post/4e5c2ca7-c1dd-48d8-af4b-f9b1adb3dd3d/image.png)


- 에러. ingress-nginx-controller가 dns주소를 제대로 받아오지 못함.  
  
✅ 올바르게 설정된 경우, 아래 순서로 동작해야 함

my-web-service가 AWS NLB에서 받은 트래픽을 Ingress Controller(NodePort)로 전달
Ingress Controller가 트래픽을 받아서 적절한 내부 서비스로 라우팅
Ingress 리소스(ingress.yaml)가 my-web-service로 요청을 전달
> 사용자 (브라우저)
  ↓
AWS NLB (80, 443)
  ↓
Kubernetes NodePort (31814, 30256)
  ↓
Ingress-NGINX Controller (80, 443) ✅ 올바른 포트로 연결됨!
  ↓
Ingress 규칙에 따라 내부 서비스로 전달
  ↓
Kubernetes Service (my-web-service)
  ↓
Pod (my-web-app)

  - 해결 방법
  
  1️⃣ Ingress-NGINX Controller 배포

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
```
  
Ingress Controller를 클러스터에 배포.

  
2️⃣ Ingress-NGINX 서비스 설정 (NLB 연동)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
spec:
  type: LoadBalancer
  externalTrafficPolicy: Local
  selector:
    app.kubernetes.io/name: ingress-nginx
  ports:
    - name: http
      nodePort: 31814
      port: 80
      protocol: TCP
      targetPort: 80  # Ingress Controller 내부 포트와 일치
    - name: https
      nodePort: 30256
      port: 443
      protocol: TCP
      targetPort: 443  # Ingress Controller 내부 포트와 일치
```
> ⚠️ 중요:
targetPort: 80, 443 → Ingress Controller가 실제로 수신하는 포트
nodePort: 31814, 30256 → NodePort를 통해 NLB가 연결되는 포트
externalTrafficPolicy: Local → 클라이언트의 실제 IP를 유지하도록 설정

  
3️⃣ 애플리케이션 서비스 (my-web-service)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-web-service
  namespace: default
spec:
  selector:
    app: my-web-app
  type: ClusterIP
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80  # 애플리케이션 Pod 내부에서 사용하는 포트
```

4️⃣ Ingress 설정
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: my-web-service  # 내부 서비스
                port:
                  number: 80
```

host: myweb.local 또는 NLB의 DNS 주소 사용 가능
nginx.ingress.kubernetes.io/rewrite-target: / → 요청 URL을 재작성하여 백엔드 서비스에 전달
service.name: my-web-service → 트래픽이 전달될 서비스
적용 명령어

  
  접속 성공..
![](https://velog.velcdn.com/images/luckyprice1103/post/c25b5113-a40b-4714-a843-3d8a69d3771b/image.png)
  
  
애플리케이션을 여러 개의 파드로 배포하고, 외부에서 접근할 수 있도록 서비스(Service)를 설정하여 정상적으로 동작할 수 있음.

![](https://velog.velcdn.com/images/luckyprice1103/post/5b1a6e0b-8483-401b-8909-78405bcde3f3/image.png)

 
### 5. 결과
  ✅ 1단계
✔ 쿠버네티스 멀티노드 클러스터 구축 완료
✔ 웹 애플리케이션 (Nginx 기반) 배포 완료
✔ 서비스(Service) 및 Ingress 설정 완료
✔ AWS LoadBalancer(NLB) 연결 완료
✔ 외부에서 웹 애플리케이션 정상 접속 확인

## 3단계: HPA(자동 확장) 설정 및 부하 테스트

목표:

애플리케이션이 높은 트래픽을 처리할 수 있도록 자동 확장(HPA) 을 설정하고,
부하 테스트를 통해 확장 동작을 검증.

쿠버네티스의 Horizontal Pod Autoscaler(HPA) 는 CPU 사용량 또는 메모리 사용량 등의 메트릭을 기반으로 Pod 개수를 자동으로 조정하는 기능.

✅ HPA 동작 방식

트래픽 부하 증가 → CPU 사용률 상승 → Pod 개수 증가 (Auto Scale-Up)
트래픽 감소 → CPU 사용률 하락 → Pod 개수 감소 (Auto Scale-Down)

✅ HPA 적용 시 이점

트래픽 급증 시 자동 확장 → 서비스 장애 예방
부하가 줄어들면 Pod 개수를 자동 축소 → 비용 절감

### 1. HPA를 위한 설정 준비
📌 1. Metrics Server 설치
HPA는 CPU/메모리 사용량을 기반으로 Pod 개수를 조정하는데, 이를 측정하는 Metrics Server가 필요.


✅ Metrics Server 설치

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

✅ TLS 인증서 문제 해결
metrics-server가 kubelet과 통신할 때 TLS 인증서 검증이 실패하여 실행되지 않는 경우가 많음.
이 문제를 해결하기 위해 TLS 검증을 비활성화하는 옵션을 추가.

```bash
kubectl patch deployment metrics-server -n kube-system --type='json' -p='[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}]'

```

설치 후 정상적으로 동작하는지 확인:

```bash
kubectl get deployment metrics-server -n kube-system
```
✅ metrics-server가 Running 상태여야 함.

![](https://velog.velcdn.com/images/luckyprice1103/post/b8a9be11-5096-42a4-ab64-c08b06730189/image.png)

📌 2. HPA 설정 (자동 확장)
이제 실제 애플리케이션에 HPA를 적용.

✅ HPA를 적용할 Deployment 확인

```bash
kubectl get deployment -n default
```


✅ HPA 설정을 위한 YAML 작성 (hpa.yaml)


```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-web-app-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-web-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50  # CPU 사용률이 50% 이상이면 확장
```
✅ HPA 적용

kubectl apply -f hpa.yaml

✅ 적용 확인
![](https://velog.velcdn.com/images/luckyprice1103/post/b2d9fd92-6c9f-49f8-9b76-a1d2504ab582/image.png)

📌 TARGETS 필드에 CPU 사용률이 나오면 정상적으로 동작!

### 2. 부하 테스트 (Load Test)

🔥 HPA가 실제로 잘 동작하는지 확인하려면 부하를 발생시켜야 한다!

📌 부하 테스트 도구 설치
Apache Bench (ab) 설치

```bash
sudo apt install apache2-utils
```

📌 부하 테스트 실행(ab 사용)


```bash
ab -n 50000 -c 100 http://(로드밸런서주소)/
-n 50000 : 총 50,000개의 요청 전송
-c 100 : 동시에 100개의 요청 실행
```
![](https://velog.velcdn.com/images/luckyprice1103/post/829f1d5d-a506-4145-b34f-1ca7c88cc09d/image.png)



📌부하 테스트 실행 후, HPA가 Pod을 추가하는지 확인.

```
kubectl get hpa -n default
```
 TARGETS 값이 올라가고, Pod 개수가 증가하는지 확인!


```
kubectl get pods -n default
```

![](https://velog.velcdn.com/images/luckyprice1103/post/57407c0c-0d9d-4824-bb5e-e303fa10da52/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/427fe6a4-9669-4ffe-9d2c-ea20ad5f23aa/image.png)
![](https://velog.velcdn.com/images/luckyprice1103/post/25525c72-9f3f-42ac-9997-4336012dbc8f/image.png)


새롭게 생성되는 my-web-app Pod들이 보이므로 HPA가 정상적으로 동작 중.


📌 부하 테스트 종료 후 HPA 감소 확인


```bash
kubectl get pods -n default -w
```
![](https://velog.velcdn.com/images/luckyprice1103/post/cb4af3e2-85e6-4398-9f5c-44a358384a37/image.png) ![](https://velog.velcdn.com/images/luckyprice1103/post/66438779-c5bc-4135-adb4-2376eced0e84/image.png)
 HPA가 자동으로 Pod 개수를 조정.


### 3. 결과

✅ 3단계
✔ Metrics Server 설치 → HPA가 CPU/메모리 데이터를 수집할 수 있도록 설정
✔ HPA 설정 및 적용 → hpa.yaml을 사용하여 Pod 자동 확장 설정
✔ 부하 테스트 진행 → ab 또는 hey를 사용하여 트래픽 증가 유발
✔ HPA 반응 확인 → kubectl get hpa & kubectl get pods로 Pod 개수 증가 확인
✔ 테스트 종료 후 Pod 감소 확인 → 트래픽 감소 시 HPA가 Pod 개수를 줄이는지 체크


## 4단계: 최종 검증 및 과제 제출

✅ 목표
쿠버네티스 클러스터가 정상적으로 동작하는지 점검하고, GitOps 기반 배포 자동화, HPA(자동 확장) 및 부하 테스트를 종합적으로 검토한 후 최적화


### 1.  클러스터 및 애플리케이션 상태 점검
쿠버네티스 클러스터가 정상적으로 동작하는지 확인합니다.


```
kubectl get nodes
kubectl get pods -A
kubectl get deployments -A
kubectl get services -A
kubectl get ingress -A
kubectl top nodes
kubectl top pods -A
```

![](https://velog.velcdn.com/images/luckyprice1103/post/96f2520f-7009-4ee5-a6dc-4dd05904562f/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/d07a1804-d689-44eb-b09b-243b42f2b6c7/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/8cc2bdb1-0594-43b7-b481-50eacf95062c/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/145fc705-7315-412d-8ec6-006f00da7d46/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/03b6cf7d-6cf1-4b7d-9e75-d20e4ff81754/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/02499df8-8552-4d1f-a1b3-34d8349868f1/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/35facb87-39c5-4ccf-bb02-f6d1f500b344/image.png)


🔹 점검 항목
✅ 모든 노드와 파드가 Ready 상태인가?
✅ 서비스 및 Ingress가 정상적으로 동작하는가?
✅ CPU 및 메모리 리소스 사용량이 적절한가?

---> 동작 확인

### 2. GitOps 배포 자동화 검증
GitOps가 제대로 동작하는지 검증합니다.

![](https://velog.velcdn.com/images/luckyprice1103/post/d237ca6d-071a-4caf-b799-35bdaba4cdb0/image.png)

```bash
kubectl get applications -n argocd
kubectl get pods -n argocd
kubectl get events -n argocd --sort-by=.metadata.creationTimestamp | tail -20
```
테스트 방법:
🔹 Git 저장소에서 Deployment의 컨테이너 이미지를 변경
🔹 변경 사항을 git push 후 자동 배포가 이루어지는지 확인
🔹 배포 실패 시 자동 롤백이 정상 동작하는지 확인


---> 동작 확인

### 3. HPA(자동 확장) 동작 검증
HPA가 정상적으로 동작하는지 확인.


```
kubectl get hpa
kubectl describe hpa my-web-app
```
HPA가 정상적으로 확장되는지 부하 테스트를 진행하여 확인

![](https://velog.velcdn.com/images/luckyprice1103/post/589d0121-eba1-4280-9c19-d77fee45f63e/image.png)


### 4. 부하 테스트 결과 분석
부하 테스트 도구(ApacheBench ab)를 사용하여 실제 시스템의 한계를 분석합니다.

```
ab -n 100000 -c 500 http://k8s-ingressn-ingressn-xxxxxxx.elb.ap-northeast-2.amazonaws.com/
```
🔹 주요 분석 항목:

응답 시간(latency)
TPS(Transactions Per Second)
성공/실패 요청 비율
오토스케일링 반응 시간


### 5. grafana로 쿠버네티스 환경 모니터링.
![](https://velog.velcdn.com/images/luckyprice1103/post/5d7e3ef9-046f-4c86-bd75-2f6c62b6b5a2/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/817b4e35-6bd1-46b8-8c4c-7d37be1ba3c0/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/ef1abd42-a325-42df-aa9a-24f94704765d/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/97753467-e6ad-4b91-89e5-8528c71a1fa7/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/890bc394-dced-47ee-a218-765dbce6a947/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/55b792af-5196-4076-a360-3d0ee98098b4/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/d634f7f6-58cf-47da-8127-e15164b17096/image.png)![](https://velog.velcdn.com/images/luckyprice1103/post/31231a4e-1bd5-4848-9d85-a8251b14f19c/image.png)


### 6. 최적화 및 개선점 도출
테스트 결과를 바탕으로 최적화 전략을 도출합니다.

✔ HPA 설정 조정

최소/최대 파드 개수 변경
CPU/MEM 사용량 임계치 조정
✔ Ingress 성능 최적화

KeepAlive 설정 조정
로드밸런서 유형 변경(NLB vs ALB)
✔ GitOps 안정화

배포 승인(Manual Approval) 추가
Canary Deployment 도입


### 7. 결과
✅ 쿠버네티스 클러스터가 정상적으로 동작하는가?
✅ GitOps 기반으로 배포 자동화가 이루어지고 있는가?
✅ 애플리케이션 변경 사항이 Git을 통해 자동 배포되는가?
✅ HPA가 부하 증가에 따라 적절하게 확장되고 있는가?
✅ 부하 테스트 결과를 통해 시스템의 한계와 최적의 확장 전략을 분석했는가?